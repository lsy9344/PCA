"""
B ë§¤ì¥ ìˆ˜ì •ëœ ë¡œì§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- ê²€ìƒ‰ ìƒíƒœ ìœ ì§€ ì²´í¬ë°•ìŠ¤ í™•ì¸ ë° í™œì„±í™”
- ì •í™•í•œ ì¿ í° ì ìš© ê°œìˆ˜ ê³„ì‚°
- A ë§¤ì¥ê³¼ ë™ì¼í•œ í¬ë§·ì˜ ë¡œê·¸ ì¶œë ¥
"""
import asyncio
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from infrastructure.config.config_manager import ConfigManager
from playwright.async_api import async_playwright
from infrastructure.web_automation.store_crawlers.b_store_crawler import BStoreCrawler
from core.domain.rules.b_discount_rule import BDiscountRule
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_b_store_updated():
    """B ë§¤ì¥ ì—…ë°ì´íŠ¸ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    # ì„¤ì • ë¡œë“œ
    config_manager = ConfigManager()
    b_config = config_manager.get_store_config("B")
    
    # B ë§¤ì¥ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = BStoreCrawler(b_config)
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ (headful ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ë™ì‘ í™•ì¸)
        browser = await p.chromium.launch(headless=False, slow_mo=1000)
        page = await browser.new_page()
        
        try:
            logger.info("ğŸš€ B ë§¤ì¥ ì—…ë°ì´íŠ¸ëœ ìë™í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            # 1. ë¡œê·¸ì¸ í…ŒìŠ¤íŠ¸ (ê²€ìƒ‰ ìƒíƒœ ìœ ì§€ ì²´í¬ë°•ìŠ¤ í¬í•¨)
            logger.info("\n=== 1ë‹¨ê³„: ë¡œê·¸ì¸ ë° ê²€ìƒ‰ ìƒíƒœ ìœ ì§€ ì²´í¬ë°•ìŠ¤ ì„¤ì • ===")
            login_success = await crawler.login(page)
            if not login_success:
                raise Exception("ë¡œê·¸ì¸ ì‹¤íŒ¨")
            
            # ë¡œê·¸ì¸ í›„ ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸
            logger.info("âœ… ë¡œê·¸ì¸ ì™„ë£Œ - ê²€ìƒ‰ ìƒíƒœ ìœ ì§€ ì²´í¬ë°•ìŠ¤ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
            
            # 2. ì¡´ì¬í•˜ëŠ” ì°¨ëŸ‰ë²ˆí˜¸ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            logger.info("\n=== 2ë‹¨ê³„: ì¡´ì¬í•˜ëŠ” ì°¨ëŸ‰ë²ˆí˜¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===")
            existing_car = "8876"  # ì¡´ì¬í•˜ëŠ” ì°¨ëŸ‰ë²ˆí˜¸
            search_success = await crawler.search_car(page, existing_car)
            if search_success:
                logger.info(f"âœ… ì°¨ëŸ‰ë²ˆí˜¸ '{existing_car}' ê²€ìƒ‰ ì„±ê³µ")
                
                # 3. ì¿ í° ì´ë ¥ ì¡°íšŒ ë° ì ìš© í…ŒìŠ¤íŠ¸
                logger.info("\n=== 3ë‹¨ê³„: ì¿ í° ì´ë ¥ ì¡°íšŒ ë° ì ìš© ===")
                my_history, total_history, discount_info = await crawler.get_coupon_history(page)
                
                # í• ì¸ ê·œì¹™ ì ìš©
                discount_rule = BDiscountRule()
                coupons_to_apply = discount_rule.decide_coupon_to_apply(my_history, total_history, discount_info)
                
                if coupons_to_apply:
                    logger.info(f"ğŸ’° ì ìš©í•  ì¿ í°: {coupons_to_apply}")
                    apply_success = await crawler.apply_coupons(page, coupons_to_apply)
                    if apply_success:
                        logger.info("âœ… ì¿ í° ì ìš© ì„±ê³µ")
                    else:
                        logger.warning("âš ï¸ ì¿ í° ì ìš© ì‹¤íŒ¨")
                else:
                    logger.info("â„¹ï¸ ì ìš©í•  ì¿ í°ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.warning(f"âš ï¸ ì°¨ëŸ‰ë²ˆí˜¸ '{existing_car}' ê²€ìƒ‰ ì‹¤íŒ¨")
            
            # 4. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì°¨ëŸ‰ë²ˆí˜¸ë¡œ í…”ë ˆê·¸ë¨ ì•Œë¦¼ í…ŒìŠ¤íŠ¸
            logger.info("\n=== 4ë‹¨ê³„: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì°¨ëŸ‰ë²ˆí˜¸ í…”ë ˆê·¸ë¨ ì•Œë¦¼ í…ŒìŠ¤íŠ¸ ===")
            nonexistent_car = "9999"  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì°¨ëŸ‰ë²ˆí˜¸
            logger.info(f"ğŸ“± ì°¨ëŸ‰ë²ˆí˜¸ '{nonexistent_car}'ë¡œ í…”ë ˆê·¸ë¨ ì•Œë¦¼ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # ìƒˆë¡œìš´ í˜ì´ì§€ íƒ­ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ê²€ìƒ‰ ìƒíƒœ ì´ˆê¸°í™”)
            new_page = await browser.new_page()
            
            # ë¡œê·¸ì¸ ë‹¤ì‹œ ìˆ˜í–‰
            login_success_2 = await crawler.login(new_page)
            if login_success_2:
                # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì°¨ëŸ‰ë²ˆí˜¸ë¡œ ê²€ìƒ‰
                search_result = await crawler.search_car(new_page, nonexistent_car)
                if not search_result:
                    logger.info("âœ… ì°¨ëŸ‰ë²ˆí˜¸ ì—†ìŒ ê°ì§€ ë° í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ ì°¨ëŸ‰ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            await new_page.close()
            
            logger.info("\nğŸ‰ B ë§¤ì¥ ì—…ë°ì´íŠ¸ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        finally:
            # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ì ì‹œ ëŒ€ê¸°
            logger.info("â±ï¸ 10ì´ˆ í›„ ë¸Œë¼ìš°ì €ë¥¼ ë‹«ìŠµë‹ˆë‹¤...")
            await asyncio.sleep(10)
            await browser.close()

if __name__ == "__main__":
    asyncio.run(test_b_store_updated()) 